{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World\n",
    "\n",
    "> The Iris dataset is the \"hello world\" of machine learning. It consists of classifying three types of Iris plant according to 4 distinct features, the sepal lenght and width and the petal lenght and width.  \n",
    "\n",
    "> We start by importing the dataset from the UCI Machine Learning Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1) # display full content of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data\"\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "iris_df = pd.read_csv(url, names=names)\n",
    "print(iris_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset has no missing values and the values are all in centimeters. There is little preprocessing to be done for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count  150.000000    150.000000   150.000000    150.000000 \n",
      "mean   5.843333      3.057333     3.758000      1.199333   \n",
      "std    0.828066      0.435866     1.765298      0.762238   \n",
      "min    4.300000      2.000000     1.000000      0.100000   \n",
      "25%    5.100000      2.800000     1.600000      0.300000   \n",
      "50%    5.800000      3.000000     4.350000      1.300000   \n",
      "75%    6.400000      3.300000     5.100000      1.800000   \n",
      "max    7.900000      4.400000     6.900000      2.500000   \n"
     ]
    }
   ],
   "source": [
    "print(iris_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(iris_df.groupby('class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      "sepal_length    150 non-null float64\n",
      "sepal_width     150 non-null float64\n",
      "petal_length    150 non-null float64\n",
      "petal_width     150 non-null float64\n",
      "class           150 non-null object\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 5.9+ KB\n"
     ]
    }
   ],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data for model validation\n",
    "> Separate 20% of the data for testing the model afterwards. Function model_selection.train_test_split() automates this process and also randomizes the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection \n",
    "array = iris_df.values\n",
    "X = array[:,0:4]\n",
    "y = array[:,4]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, y_train, y_validation = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Test models\n",
    "   \n",
    "   - Two different algorithms will be tested, one linear and one nonlinear. The difference between them is that linear models make more rigid assumptions about the data. The linear algorithm tested will be the Logistic Regression (LR) and the nonlinear will be the Decision Tree Classifier (CART).   \n",
    "   - Acuracy will be used as an evaluation metric. It simply divides the number of correctly predicted instances by the total number of instances, outputting a percentage.   \n",
    "   - The data used to train the model, X_train, will be split in 10 parts, trained on 9 and tested on 1, for all combinations. This is the process of Cross-Validation. The accuracy of each model will then be the mean of the accuracy for all combinations.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.966667 \n",
      "CART: 0.983333 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "seed = 3\n",
    "scoring = 'accuracy'\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f \" % (name, cv_results.mean())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Decision Tree Classifier reached a 98.33% accuracy so it will be used to predict the 20% left of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "DTC.fit(X_train, y_train)\n",
    "predictions = DTC.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reaching a 86.6% accuracy on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
